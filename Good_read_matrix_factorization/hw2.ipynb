{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25475\n",
      "18892\n",
      "(25475, 18892)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import gzip\n",
    "import numpy as np\n",
    "filename = 'goodreads_reviews_spoiler.json.gz'\n",
    "\n",
    "A = np.zeros((25475, 18892), dtype=np.float64)\n",
    "book_idx = {'next_idx': 0}\n",
    "user_idx = {'next_idx': 0}\n",
    "with gzip.open(filename, 'rb') as ds:\n",
    "    for line in ds:\n",
    "        line = line.rstrip()\n",
    "        if line:\n",
    "            obj = json.loads(line)\n",
    "            book_id = str(obj['book_id'])\n",
    "            user_id = str(obj['user_id'])\n",
    "            if book_id not in book_idx.keys():\n",
    "                book_idx[book_id] = book_idx['next_idx']\n",
    "                book_idx['next_idx'] += 1\n",
    "            if user_id not in user_idx.keys():\n",
    "                user_idx[user_id] = user_idx['next_idx']\n",
    "                user_idx['next_idx'] += 1\n",
    "            row = book_idx[book_id]\n",
    "            col = user_idx[user_id]\n",
    "            A[row][col] = obj['rating']\n",
    "print(book_idx['next_idx'])\n",
    "print(user_idx['next_idx'])\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(77,) (34,)\n",
      "iteration: 0, loss: 430246.3476, difference: 429875.4317, W_norm: 282.7736, Z_norm: 88.1422\n",
      "iteration: 1, loss: 346581.8163, difference: 346265.0896, W_norm: 259.5847, Z_norm: 57.1420\n",
      "iteration: 2, loss: 312144.0836, difference: 311839.8627, W_norm: 253.2707, Z_norm: 50.9502\n",
      "iteration: 3, loss: 291022.4738, difference: 290721.6652, W_norm: 252.6587, Z_norm: 48.1498\n",
      "iteration: 4, loss: 276018.0653, difference: 275718.4941, W_norm: 252.7211, Z_norm: 46.8501\n",
      "iteration: 5, loss: 264549.5967, difference: 264249.0559, W_norm: 253.7006, Z_norm: 46.8402\n",
      "iteration: 6, loss: 255402.2695, difference: 255099.9574, W_norm: 254.9586, Z_norm: 47.3535\n",
      "iteration: 7, loss: 247862.2036, difference: 247557.6252, W_norm: 256.6140, Z_norm: 47.9645\n",
      "iteration: 8, loss: 241501.3605, difference: 241194.3330, W_norm: 258.1396, Z_norm: 48.8879\n",
      "iteration: 9, loss: 236030.8272, difference: 235721.3526, W_norm: 259.5771, Z_norm: 49.8975\n",
      "iteration: 10, loss: 231263.5012, difference: 230951.8282, W_norm: 260.7866, Z_norm: 50.8864\n",
      "iteration: 11, loss: 227044.2784, difference: 226730.5391, W_norm: 262.0494, Z_norm: 51.6899\n",
      "iteration: 12, loss: 223273.1361, difference: 222957.5227, W_norm: 263.3716, Z_norm: 52.2418\n",
      "iteration: 13, loss: 219879.8415, difference: 219562.4279, W_norm: 264.8113, Z_norm: 52.6023\n",
      "iteration: 14, loss: 216801.8680, difference: 216482.8658, W_norm: 266.0550, Z_norm: 52.9472\n",
      "iteration: 15, loss: 213989.7401, difference: 213668.9729, W_norm: 267.3903, Z_norm: 53.3768\n",
      "iteration: 16, loss: 211409.0782, difference: 211086.3538, W_norm: 268.7690, Z_norm: 53.9554\n",
      "iteration: 17, loss: 209028.5966, difference: 208703.9725, W_norm: 270.1149, Z_norm: 54.5092\n",
      "iteration: 18, loss: 206823.7102, difference: 206497.3080, W_norm: 271.4095, Z_norm: 54.9927\n",
      "iteration: 19, loss: 204772.6310, difference: 204444.4216, W_norm: 272.6896, Z_norm: 55.5199\n",
      "iteration: 20, loss: 202857.3734, difference: 202527.5178, W_norm: 273.8355, Z_norm: 56.0201\n",
      "iteration: 21, loss: 201060.5238, difference: 200729.0418, W_norm: 275.0093, Z_norm: 56.4727\n",
      "iteration: 22, loss: 199370.3568, difference: 199037.3844, W_norm: 276.1471, Z_norm: 56.8253\n",
      "iteration: 23, loss: 197778.1212, difference: 197443.6135, W_norm: 277.2753, Z_norm: 57.2324\n",
      "iteration: 24, loss: 196271.1886, difference: 195935.1371, W_norm: 278.4779, Z_norm: 57.5735\n",
      "iteration: 25, loss: 194843.4527, difference: 194505.8370, W_norm: 279.6301, Z_norm: 57.9856\n",
      "iteration: 26, loss: 193490.6817, difference: 193151.4516, W_norm: 280.7838, Z_norm: 58.4463\n",
      "iteration: 27, loss: 192206.7966, difference: 191866.0614, W_norm: 281.8546, Z_norm: 58.8806\n",
      "iteration: 28, loss: 190985.8087, difference: 190643.6612, W_norm: 282.9285, Z_norm: 59.2190\n",
      "iteration: 29, loss: 189822.3688, difference: 189478.7793, W_norm: 283.9351, Z_norm: 59.6543\n",
      "iteration: 30, loss: 188709.6569, difference: 188364.7954, W_norm: 284.8658, Z_norm: 59.9957\n",
      "iteration: 31, loss: 187646.2695, difference: 187300.0440, W_norm: 285.7957, Z_norm: 60.4299\n",
      "iteration: 32, loss: 186627.1265, difference: 186279.5428, W_norm: 286.7651, Z_norm: 60.8186\n",
      "iteration: 33, loss: 185650.1858, difference: 185301.3980, W_norm: 287.6178, Z_norm: 61.1699\n",
      "iteration: 34, loss: 184714.0012, difference: 184364.0021, W_norm: 288.4496, Z_norm: 61.5496\n",
      "iteration: 35, loss: 183814.9143, difference: 183463.7967, W_norm: 289.2291, Z_norm: 61.8884\n",
      "iteration: 36, loss: 182950.6979, difference: 182598.3540, W_norm: 290.0134, Z_norm: 62.3304\n",
      "iteration: 37, loss: 182117.7308, difference: 181764.2914, W_norm: 290.8254, Z_norm: 62.6139\n",
      "iteration: 38, loss: 181315.5552, difference: 180960.9486, W_norm: 291.6191, Z_norm: 62.9875\n",
      "iteration: 39, loss: 180543.3422, difference: 180187.6414, W_norm: 292.3615, Z_norm: 63.3394\n",
      "iteration: 40, loss: 179797.9286, difference: 179441.1361, W_norm: 293.0755, Z_norm: 63.7169\n",
      "iteration: 41, loss: 179078.5846, difference: 178720.8128, W_norm: 293.7160, Z_norm: 64.0557\n",
      "iteration: 42, loss: 178383.2054, difference: 178024.5405, W_norm: 294.3373, Z_norm: 64.3277\n",
      "iteration: 43, loss: 177710.6631, difference: 177351.0614, W_norm: 294.9602, Z_norm: 64.6416\n",
      "iteration: 44, loss: 177059.1282, difference: 176698.5412, W_norm: 295.6212, Z_norm: 64.9658\n",
      "iteration: 45, loss: 176428.2915, difference: 176066.7535, W_norm: 296.2549, Z_norm: 65.2832\n",
      "iteration: 46, loss: 175816.3186, difference: 175453.8179, W_norm: 296.9081, Z_norm: 65.5927\n",
      "iteration: 47, loss: 175222.1976, difference: 174858.8079, W_norm: 297.5659, Z_norm: 65.8238\n",
      "iteration: 48, loss: 174646.1966, difference: 174281.9203, W_norm: 298.2081, Z_norm: 66.0682\n",
      "iteration: 49, loss: 174086.5676, difference: 173721.4533, W_norm: 298.8203, Z_norm: 66.2940\n"
     ]
    }
   ],
   "source": [
    "def get_bjs(A):\n",
    "    # return the nonzero rows\n",
    "    row_idxes, col_idxes = np.nonzero(A)\n",
    "    bj_s = []\n",
    "    indexes_s = []\n",
    "    indexes = []\n",
    "    k = 0\n",
    "    while k < len(row_idxes):\n",
    "        bj = []\n",
    "        indexes = []\n",
    "        row_idx = len(bj_s)\n",
    "        while k < len(row_idxes) and row_idxes[k] == row_idx:\n",
    "            bj.append(A[row_idx][col_idxes[k]])\n",
    "            indexes.append(col_idxes[k])\n",
    "            k += 1\n",
    "        bj_s.append(np.array(bj))\n",
    "        indexes_s.append(indexes)\n",
    "    return bj_s, indexes_s\n",
    "\n",
    "def get_wj(W, indexes, m):\n",
    "    # according to the indexes pick corresponding rows of W\n",
    "    mask = np.zeros((len(indexes), m))\n",
    "    for i in range(len(indexes)):\n",
    "        mask[i][indexes[i]] = 1\n",
    "    return np.dot(mask, W)\n",
    "\n",
    "def compute_loss(W, Z, A, indexes_W, beta):\n",
    "    # compute the loss functions\n",
    "    app = W.dot(Z.transpose())\n",
    "    # keep elements with positions correponding to the nonzero elements in A\n",
    "    app2 = np.zeros(app.shape)\n",
    "    for j in range(len(indexes_W)):\n",
    "        for i in indexes_W[j]:\n",
    "            app2[i][j] = app[i][j]\n",
    "    term1 = np.linalg.norm(A - app2) ** 2\n",
    "    term2 = beta * np.linalg.norm(W) ** 2\n",
    "    term3 = beta * np.linalg.norm(Z) ** 2\n",
    "    loss =  term1 + term2 + term3 \n",
    "    return loss, term1, term2, term3\n",
    "    \n",
    "\n",
    "aj_s, indexes_W = get_bjs(A.transpose())\n",
    "bj_s, indexes_Z = get_bjs(A)\n",
    "\n",
    "print(aj_s[0].shape, bj_s[0].shape)\n",
    "\n",
    "# Hyperparameters\n",
    "k = 32\n",
    "beta = 1e-4\n",
    "iterations = 50\n",
    "\n",
    "m = 25475 # number of books\n",
    "n = 18892 # number of users\n",
    "\n",
    "# Initialize W and Z\n",
    "W = np.random.rand(m, k) + 1\n",
    "Z = np.random.rand(n, k) + 1\n",
    "\n",
    "# start ALS algorithm\n",
    "for iteration in range(iterations):\n",
    "    # Fix W, optimize Z\n",
    "    for j in range(n):\n",
    "        aj = aj_s[j]\n",
    "        Wj = get_wj(W, indexes_W[j], m)\n",
    "        B = Wj.transpose().dot(Wj) + beta * np.eye(k)\n",
    "        s = aj.transpose().dot(Wj)\n",
    "        zj = np.linalg.solve(B, s)\n",
    "        Z[j, :] = zj.squeeze()\n",
    "    # Fix Z, optimize W\n",
    "    for j in range(m):\n",
    "        bj = bj_s[j]\n",
    "        Zj = get_wj(Z, indexes_Z[j], n)\n",
    "        B = Zj.transpose().dot(Zj) + beta * np.eye(k)\n",
    "        s = bj.transpose().dot(Zj)\n",
    "        wj = np.linalg.solve(B, s)\n",
    "        W[j, :] = wj.squeeze()\n",
    "    loss, difference, W_norm, Z_norm = compute_loss(W, Z, A, indexes_W, beta)\n",
    "    print('iteration: {}, loss: {:.4f}, difference: {:.4f}, W_norm: {:.4f}, Z_norm: {:.4f}'.\n",
    "          format(iteration, loss, difference, W_norm, Z_norm))\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25475\n",
      "book id: 2\n",
      "book id: 1\n",
      "book id: 5\n",
      "book id: 15881\n",
      "book id: 6\n",
      "book id: 136251\n"
     ]
    }
   ],
   "source": [
    "target_book_id = 2 \n",
    "target_row = book_idx[str(target_book_id)]\n",
    "target_embedding = W[target_row, :]\n",
    "# construct a row idx to book id dictionary (list)\n",
    "row_to_book_id = [0] * m\n",
    "print(len(row_to_book_id))\n",
    "for key in book_idx.keys():\n",
    "    if key == 'next_idx':\n",
    "        continue\n",
    "    row = book_idx[key]\n",
    "    row_to_book_id[row] = key\n",
    "\n",
    "# find 5 books that are closest to target book\n",
    "mask = np.eye(m)\n",
    "mask[:, target_row] -= np.ones(m)\n",
    "compare_embedding = mask.dot(W)\n",
    "norm_vec = np.linalg.norm(compare_embedding, ord=2, axis=1)\n",
    "closest_order = np.argsort(norm_vec)\n",
    "for i in range(6):\n",
    "    row_idx = closest_order[i]\n",
    "    print('book id: {}'.format(row_to_book_id[row_idx]))\n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
